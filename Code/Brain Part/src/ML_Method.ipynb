{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: EMD-signal in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (1.6.4)\n",
      "Requirement already satisfied: numpy>=1.12 in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (from EMD-signal) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (from EMD-signal) (1.11.4)\n",
      "Requirement already satisfied: pathos>=0.2.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (from EMD-signal) (0.3.3)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.64.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (from EMD-signal) (4.66.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (from pathos>=0.2.1->EMD-signal) (1.7.6.9)\n",
      "Requirement already satisfied: dill>=0.3.9 in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (from pathos>=0.2.1->EMD-signal) (0.3.9)\n",
      "Requirement already satisfied: pox>=0.3.5 in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (from pathos>=0.2.1->EMD-signal) (0.3.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.17 in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (from pathos>=0.2.1->EMD-signal) (0.70.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\aniru\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0,>=4.64.0->EMD-signal) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install EMD-signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import antropy as ant\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from scipy.signal import welch\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PyEMD import EMD\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.linalg import svd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a12 = pd.read_csv(\"../Dataset/as/ones/a12.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "a13 = pd.read_csv(\"../Dataset/as/ones/a13.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "a11 = pd.read_csv(\"../Dataset/as/ones/Letter_A.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "a21 = pd.read_csv(\"../Dataset/as/twos/a21.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "a4 = pd.read_csv(\"../Dataset/as/fours/a4.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "\n",
    "as_1 = pd.read_csv(\"../Dataset/dataset/as_1.csv\").dropna().iloc[:2304, 1:9].to_numpy()\n",
    "\n",
    "ak_1 = pd.read_csv(\"../Dataset/A11_K.csv\").dropna().iloc[0:12032, 1:9].to_numpy()\n",
    "ak_2 = pd.read_csv(\"../Dataset/A12_K.csv\").dropna().iloc[0:12032, 1:9].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b12 = pd.read_csv(\"../Dataset/bs/ones/b12.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "b13 = pd.read_csv(\"../Dataset/bs/ones/b13.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "b11 = pd.read_csv(\"../Dataset/bs/ones/Letter_B.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "b31 = pd.read_csv(\"../Dataset/bs/threes/b31.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "b4 = pd.read_csv(\"../Dataset/bs/fours/b4.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "\n",
    "bs_1 = pd.read_csv(\"../Dataset/dataset/bs_1.csv\").dropna().iloc[:2304, 1:9].to_numpy()\n",
    "\n",
    "bk_1 = pd.read_csv(\"../Dataset/B11_K.csv\").dropna().iloc[:12032, 1:9].to_numpy()\n",
    "bk_2 = pd.read_csv(\"../Dataset/B12_K.csv\").dropna().iloc[:12032, 1:9].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base1s = pd.read_csv(\"../Dataset/baselines/Baseline_EEG.csv\").dropna().iloc[:, 1:9]\n",
    "base2s = pd.read_csv(\"../Dataset/baselines/rest2.csv\").dropna().iloc[:, 1:9]\n",
    "base3s = pd.read_csv(\"../Dataset/baselines/rest3.csv\").dropna().iloc[:, 1:9]\n",
    "base4s = pd.read_csv(\"../Dataset/baselines/base4.csv\").dropna().iloc[:, 1:9]\n",
    "\n",
    "\n",
    "base1s_1 = pd.read_csv(\"../Dataset/dataset/rest_s1.csv\").dropna().iloc[:, 1:9]\n",
    "\n",
    "base1k_1 = pd.read_csv(\"../Dataset/rest_eeg_K.csv\").dropna().iloc[:, 1:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = base1s.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std_1 = base1s.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean_2 = base2s.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std_2 = base2s.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean_3 = base3s.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std_3 = base3s.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean_4 = base4s.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std_4 = base4s.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean1_s = base1s_1.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std1_s = base1s_1.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean1_k = base1k_1.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std1_k = base1k_1.describe().iloc[2].to_numpy().reshape((1, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseLine Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a12_new = np.zeros((19, 256, 8))\n",
    "a13_new = np.zeros((19, 256, 8))\n",
    "a11_new = np.zeros((19, 256, 8))\n",
    "a21_new = np.zeros((19, 256, 8))\n",
    "a4_new = np.zeros((19, 256, 8))\n",
    "as_new = np.zeros((9, 256, 8))\n",
    "ak1_new = np.zeros((46, 256, 8))\n",
    "ak2_new = np.zeros((46, 256, 8))\n",
    "\n",
    "b12_new = np.zeros((19, 256, 8))\n",
    "b13_new = np.zeros((19, 256, 8))\n",
    "b11_new = np.zeros((19, 256, 8))\n",
    "b31_new = np.zeros((19, 256, 8))\n",
    "b4_new = np.zeros((19, 256, 8))\n",
    "bs_new = np.zeros((9, 256, 8))\n",
    "bk1_new = np.zeros((46, 256, 8))\n",
    "bk2_new = np.zeros((46, 256, 8))\n",
    "\n",
    "for i in range(19):\n",
    "    for j in range(256):\n",
    "        for k in range(8):\n",
    "            a12_new[i, j, k] = (a12[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            a13_new[i, j, k] = (a13[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            a11_new[i, j, k] = (a11[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            a21_new[i, j, k] = (a21[i+j, k] - mean_2[0, k])/std_2[0, k]\n",
    "            a4_new[i, j, k] = (a4[i+j, k] - mean_4[0, k])/std_4[0, k]\n",
    "\n",
    "\n",
    "\n",
    "            b12_new[i, j, k] = (b12[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            b13_new[i, j, k] = (b13[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            b11_new[i, j, k] = (b11[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            b31_new[i, j, k] = (b31[i+j, k] - mean_3[0, k])/std_3[0, k]\n",
    "            b4_new[i, j, k] = (b4[i+j, k] - mean_4[0, k])/std_4[0, k]\n",
    "\n",
    "for i in range(9):\n",
    "    for j in range(256):\n",
    "        for k in range(8):\n",
    "            as_new[i, j, k] = (as_1[i+j, k] - mean1_s[0, k])/std1_s[0, k]\n",
    "            bs_new[i, j, k] = (bs_1[i+j, k] - mean1_s[0, k])/std1_s[0, k]\n",
    "\n",
    "\n",
    "for i in range(46):\n",
    "    for j in range(256):\n",
    "        for k in range(8):\n",
    "            ak1_new[i, j, k] = (ak_1[i+j, k] - mean1_k[0, k])/std1_k[0, k]\n",
    "            ak2_new[i, j, k] = (ak_2[i+j, k] - mean1_k[0, k])/std1_k[0, k]\n",
    "\n",
    "            bk1_new[i, j, k] = (bk_1[i+j, k] - mean1_k[0, k])/std1_k[0, k]\n",
    "            bk2_new[i, j, k] = (bk_2[i+j, k] - mean1_k[0, k])/std1_k[0, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_a_s = np.concatenate((a12_new, a13_new, a11_new, a21_new, a4_new, as_new, ak1_new, ak2_new), axis = 0)\n",
    "all_b_s = np.concatenate((b12_new, b13_new, b11_new, b31_new, b4_new, bs_new, bk1_new, bk2_new), axis = 0)\n",
    "\n",
    "all_a_s = all_a_s.reshape((196, 256, 8))\n",
    "all_b_s = all_b_s.reshape((196, 256, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emd_signals(input_data):\n",
    "    imf_count = 7\n",
    "    emd = EMD()\n",
    "    #input data is of the size: (196,256,8)\n",
    "    #For EMD, the input data will be of the form: (trails, time_points, channels)\n",
    "    #We get the data info first:\n",
    "    trails, time_points, channels = input_data.shape\n",
    "    imfs = np.zeros((trails, channels, imf_count, time_points))\n",
    "    # the output data will be of the type: (trials, channels, num_imfs, time points)\n",
    "    \n",
    "    for i in range(trails):\n",
    "        for j in range(channels):\n",
    "            signal = input_data[i,:,j]\n",
    "            imf_val = emd(signal)\n",
    "            \n",
    "            #For safety reasons, we will pad with zeros incase the number of IMFs obtained are lesser than what we need\n",
    "            \n",
    "            if len(imf_val) < imf_count:\n",
    "                imf_padded = np.pad(imf_val, ((0, imf_count - len(imf_val)), (0,0)), mode = 'constant')\n",
    "            else:\n",
    "                imf_padded = imf_val[:imf_count]\n",
    "            \n",
    "            imfs[i,j] = imf_padded\n",
    "    return imfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_s = emd_signals(np.array(all_a_s))\n",
    "b_s = emd_signals(np.array(all_b_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emd_features(imfs):\n",
    "    #We take the imf data for all the signals and get the statisitcal features which are:\n",
    "    # mean, variance, energy and entropy.\n",
    "    # these may seem random but all of them are generally used along with skewness and kurtosis for effective feature extraction in non stationary signals\n",
    "    #The input data is of the form: (trials, channels, num_imfs, time points)\n",
    "    \n",
    "    trials, channels, num_imfs, time_points = imfs.shape\n",
    "    features = []\n",
    "    \n",
    "    for i in range(trials):\n",
    "        trail_feature = []\n",
    "        for j in range(channels):\n",
    "            for k in range(num_imfs):\n",
    "                imf = imfs[i,j,k]\n",
    "                \n",
    "                energy = np.sum(imf**2)\n",
    "                entropy = -np.sum(imf * np.log2(np.abs(imf) + 1e-12))\n",
    "                mean = np.mean(imf)\n",
    "                variance = np.var(imf)\n",
    "                \n",
    "                trail_feature.extend([energy,entropy,mean,variance])\n",
    "        \n",
    "        features.append(trail_feature)\n",
    "        \n",
    "    return np.array(features)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_a = emd_features(a_s)\n",
    "features_b = emd_features(b_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((features_a,features_b))\n",
    "\n",
    "y = np.array([0]*len(features_a) + [1]*len(features_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_train_ML\", 'wb') as fp:\n",
    "    pickle.dump(X_train, fp)\n",
    "\n",
    "with open(\"X_test_ML\", 'wb') as fp:\n",
    "    pickle.dump(X_test, fp)\n",
    "\n",
    "with open(\"y_train_ML\", 'wb') as fp:\n",
    "    pickle.dump(y_train, fp)\n",
    "\n",
    "with open(\"y_test_ML\", 'wb') as fp:\n",
    "    pickle.dump(y_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = svm.SVC()\n",
    "model_svm.fit(X_train, y_train)\n",
    "model_pkl_file = \"SVM_MODEL.pkl\"  \n",
    "\n",
    "# with open(model_pkl_file, 'wb') as file:  \n",
    "#     pickle.dump(model_svm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model_svm.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred)) \n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0, 1])\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "model_forest.fit(X_train, y_train)\n",
    "model_pkl_file = \"RANDOM_FOREST_MODEL.pkl\"  \n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model_svm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model_forest.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred)) \n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0, 1])\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
