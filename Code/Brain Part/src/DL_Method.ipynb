{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install EMD-signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PyEMD import EMD\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import RecurrencePlot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import optimizers, layers, models, utils\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a12 = pd.read_csv(\"./Dataset/as/ones/a12.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "a13 = pd.read_csv(\"./Dataset/as/ones/a13.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "a11 = pd.read_csv(\"./Dataset/as/ones/Letter_A.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "a21 = pd.read_csv(\"./Dataset/as/twos/a21.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "a4 = pd.read_csv(\"./Dataset/as/fours/a4.csv\").iloc[:4864, 1:9].dropna().to_numpy()\n",
    "\n",
    "as_1 = pd.read_csv(\"./Dataset/dataset/as_1.csv\").dropna().iloc[:2304, 1:9].to_numpy()\n",
    "\n",
    "ak_1 = pd.read_csv(\"./Dataset/A11_K.csv\").dropna().iloc[0:12032, 1:9].to_numpy()\n",
    "ak_2 = pd.read_csv(\"./Dataset/A12_K.csv\").dropna().iloc[0:12032, 1:9].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b12 = pd.read_csv(\"./Dataset/bs/ones/b12.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "b13 = pd.read_csv(\"./Dataset/bs/ones/b13.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "b11 = pd.read_csv(\"./Dataset/bs/ones/Letter_B.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "b31 = pd.read_csv(\"./Dataset/bs/threes/b31.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "b4 = pd.read_csv(\"./Dataset/bs/fours/b4.csv\").dropna().iloc[:4864, 1:9].to_numpy()\n",
    "\n",
    "bs_1 = pd.read_csv(\"./Dataset/dataset/bs_1.csv\").dropna().iloc[:2304, 1:9].to_numpy()\n",
    "\n",
    "bk_1 = pd.read_csv(\"./Dataset/B11_K.csv\").dropna().iloc[:12032, 1:9].to_numpy()\n",
    "bk_2 = pd.read_csv(\"./Dataset/B12_K.csv\").dropna().iloc[:12032, 1:9].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base1s = pd.read_csv(\"./Dataset/baselines/Baseline_EEG.csv\").dropna().iloc[:, 1:9]\n",
    "base2s = pd.read_csv(\"./Dataset/baselines/rest2.csv\").dropna().iloc[:, 1:9]\n",
    "base3s = pd.read_csv(\"./Dataset/baselines/rest3.csv\").dropna().iloc[:, 1:9]\n",
    "base4s = pd.read_csv(\"./Dataset/baselines/base4.csv\").dropna().iloc[:, 1:9]\n",
    "\n",
    "\n",
    "base1s_1 = pd.read_csv(\"./Dataset/dataset/rest_s1.csv\").dropna().iloc[:, 1:9]\n",
    "\n",
    "base1k_1 = pd.read_csv(\"./Dataset/rest_eeg_K.csv\").dropna().iloc[:, 1:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = base1s.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std_1 = base1s.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean_2 = base2s.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std_2 = base2s.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean_3 = base3s.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std_3 = base3s.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean_4 = base4s.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std_4 = base4s.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean1_s = base1s_1.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std1_s = base1s_1.describe().iloc[2].to_numpy().reshape((1, 8))\n",
    "\n",
    "mean1_k = base1k_1.describe().iloc[1].to_numpy().reshape((1, 8))\n",
    "std1_k = base1k_1.describe().iloc[2].to_numpy().reshape((1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a12_new = np.zeros((19, 256, 8))\n",
    "a13_new = np.zeros((19, 256, 8))\n",
    "a11_new = np.zeros((19, 256, 8))\n",
    "a21_new = np.zeros((19, 256, 8))\n",
    "a4_new = np.zeros((19, 256, 8))\n",
    "as_new = np.zeros((9, 256, 8))\n",
    "ak1_new = np.zeros((46, 256, 8))\n",
    "ak2_new = np.zeros((46, 256, 8))\n",
    "\n",
    "b12_new = np.zeros((19, 256, 8))\n",
    "b13_new = np.zeros((19, 256, 8))\n",
    "b11_new = np.zeros((19, 256, 8))\n",
    "b31_new = np.zeros((19, 256, 8))\n",
    "b4_new = np.zeros((19, 256, 8))\n",
    "bs_new = np.zeros((9, 256, 8))\n",
    "bk1_new = np.zeros((46, 256, 8))\n",
    "bk2_new = np.zeros((46, 256, 8))\n",
    "\n",
    "for i in range(19):\n",
    "    for j in range(256):\n",
    "        for k in range(8):\n",
    "            a12_new[i, j, k] = (a12[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            a13_new[i, j, k] = (a13[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            a11_new[i, j, k] = (a11[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            a21_new[i, j, k] = (a21[i+j, k] - mean_2[0, k])/std_2[0, k]\n",
    "            a4_new[i, j, k] = (a4[i+j, k] - mean_4[0, k])/std_4[0, k]\n",
    "\n",
    "\n",
    "\n",
    "            b12_new[i, j, k] = (b12[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            b13_new[i, j, k] = (b13[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            b11_new[i, j, k] = (b11[i+j, k] - mean_1[0, k])/std_1[0, k]\n",
    "            b31_new[i, j, k] = (b31[i+j, k] - mean_3[0, k])/std_3[0, k]\n",
    "            b4_new[i, j, k] = (b4[i+j, k] - mean_4[0, k])/std_4[0, k]\n",
    "\n",
    "for i in range(9):\n",
    "    for j in range(256):\n",
    "        for k in range(8):\n",
    "            as_new[i, j, k] = (as_1[i+j, k] - mean1_s[0, k])/std1_s[0, k]\n",
    "            bs_new[i, j, k] = (bs_1[i+j, k] - mean1_s[0, k])/std1_s[0, k]\n",
    "\n",
    "\n",
    "for i in range(46):\n",
    "    for j in range(256):\n",
    "        for k in range(8):\n",
    "            ak1_new[i, j, k] = (ak_1[i+j, k] - mean1_k[0, k])/std1_k[0, k]\n",
    "            ak2_new[i, j, k] = (ak_2[i+j, k] - mean1_k[0, k])/std1_k[0, k]\n",
    "\n",
    "            bk1_new[i, j, k] = (bk_1[i+j, k] - mean1_k[0, k])/std1_k[0, k]\n",
    "            bk2_new[i, j, k] = (bk_2[i+j, k] - mean1_k[0, k])/std1_k[0, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_a_s = np.concatenate((a12_new, a13_new, a11_new, a21_new, a4_new, as_new, ak1_new, ak2_new), axis = 0)\n",
    "all_b_s = np.concatenate((b12_new, b13_new, b11_new, b31_new, b4_new, bs_new, bk1_new, bk2_new), axis = 0)\n",
    "\n",
    "all_a_s = all_a_s.reshape((196, 256, 8))\n",
    "all_b_s = all_b_s.reshape((196, 256, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emd_signals(input_data):\n",
    "    imf_count = 7\n",
    "    emd = EMD()\n",
    "    #input data is of the size: (196,256,8)\n",
    "    #For EMD, the input data will be of the form: (trails, time_points, channels)\n",
    "    #We get the data info first:\n",
    "    trails, time_points, channels = input_data.shape\n",
    "    imfs = np.zeros((trails, channels, imf_count, time_points))\n",
    "    # the output data will be of the type: (trials, channels, num_imfs, time points)\n",
    "    \n",
    "    for i in range(trails):\n",
    "        for j in range(channels):\n",
    "            signal = input_data[i,:,j]\n",
    "            imf_val = emd(signal)\n",
    "            \n",
    "            #For safety reasons, we will pad with zeros incase the number of IMFs obtained are lesser than what we need\n",
    "            \n",
    "            if len(imf_val) < imf_count:\n",
    "                imf_padded = np.pad(imf_val, ((0, imf_count - len(imf_val)), (0,0)), mode = 'constant')\n",
    "            else:\n",
    "                imf_padded = imf_val[:imf_count]\n",
    "            \n",
    "            imfs[i,j] = imf_padded\n",
    "    return imfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_s1 = emd_signals(np.array(all_a_s))\n",
    "b_s1 = emd_signals(np.array(all_b_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emd_features(imfs):\n",
    "    #We take the imf data for all the signals and get the statisitcal features which are:\n",
    "    # mean, variance, energy and entropy.\n",
    "    # these may seem random but all of them are generally used along with skewness and kurtosis for effective feature extraction in non stationary signals\n",
    "    #The input data is of the form: (trials, channels, num_imfs, time points)\n",
    "    \n",
    "    trials, channels, num_imfs, time_points = imfs.shape\n",
    "    features = []\n",
    "    \n",
    "    for i in range(trials):\n",
    "        trail_feature = []\n",
    "        for j in range(channels):\n",
    "            for k in range(num_imfs):\n",
    "                imf = imfs[i,j,k]\n",
    "                \n",
    "                energy = np.sum(imf**2)\n",
    "                entropy = -np.sum(imf * np.log2(np.abs(imf) + 1e-12))\n",
    "                mean = np.mean(imf)\n",
    "                variance = np.var(imf)\n",
    "                \n",
    "                trail_feature.extend([energy,entropy,mean,variance])\n",
    "        \n",
    "        features.append(trail_feature)\n",
    "        \n",
    "    return np.array(features)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_a1 = emd_features(a_s1)\n",
    "features_b1 = emd_features(b_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_a = features_a1.reshape((196,32,7))\n",
    "features_b = features_b1.reshape((196,32,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reccurence_plot(input_mats):\n",
    "    out_mats = []\n",
    "    for i in range(input_mats.shape[0]):\n",
    "        transformer = RecurrencePlot()\n",
    "        X_new = transformer.transform(input_mats[i, :, :])\n",
    "        out_mats.append(X_new)\n",
    "    return np.array(out_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_a_s = reccurence_plot(np.array(features_a))\n",
    "image_b_s = reccurence_plot(np.array(features_b))\n",
    "X = [image_a_s, image_b_s]\n",
    "X = np.array(X).reshape(392, 1, 32, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_output = 2\n",
    "y = np.array([[0]*196, [1]*196]).reshape((392,))\n",
    "y = utils.to_categorical(y, num_of_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=24)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8, test_size=0.2, random_state=16)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_train_DL\", 'wb') as fp:\n",
    "    pickle.dump(X_train, fp)\n",
    "\n",
    "with open(\"X_test_DL\", 'wb') as fp:\n",
    "    pickle.dump(X_test, fp)\n",
    "\n",
    "with open(\"X_val_DL\", 'wb') as fp:\n",
    "    pickle.dump(X_val, fp)\n",
    "\n",
    "with open(\"y_train_DL\", 'wb') as fp:\n",
    "    pickle.dump(y_train, fp)\n",
    "\n",
    "with open(\"y_test_DL\", 'wb') as fp:\n",
    "    pickle.dump(y_test, fp)\n",
    "\n",
    "with open(\"y_val_DL\", 'wb') as fp:\n",
    "    pickle.dump(y_val, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape (make sure it matches your data shape)\n",
    "input_shape = (1, 32, 7, 7)\n",
    "\n",
    "# RMSprop optimizer\n",
    "opt = optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07\n",
    ")\n",
    "\n",
    "\n",
    "# Model architecture\n",
    "input_1 = layers.Input(shape=input_shape, name='input_1')\n",
    "permuted_input = layers.Permute((1, 3, 4, 2), name='permuted_input')(input_1)\n",
    "\n",
    "# Conv3D layers\n",
    "conv3d_1 = layers.Conv3D(filters=16, kernel_size=(8, 5, 5), padding='same', activation='relu', kernel_initializer=\"glorot_uniform\", name='conv3d_1')(permuted_input)\n",
    "batch_1 = layers.BatchNormalization(name='batch_1')(conv3d_1)\n",
    "maxpool_1 = layers.MaxPool3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same', name='maxpool_1')(batch_1)\n",
    "\n",
    "conv3d_2 = layers.Conv3D(filters=8, kernel_size=(4, 4, 4), padding='same', activation='relu', kernel_initializer=\"glorot_uniform\", name='conv3d_2')(maxpool_1)\n",
    "batch_2 = layers.BatchNormalization(name='batch_2')(conv3d_2)\n",
    "maxpool_2 = layers.MaxPool3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same', name='maxpool_2')(batch_2)\n",
    "\n",
    "conv3d_3 = layers.Conv3D(filters=8, kernel_size=(4, 3, 3), padding='same', activation='relu', kernel_initializer=\"glorot_uniform\", name='conv3d_3')(maxpool_2)\n",
    "batch_3 = layers.BatchNormalization(name='batch_3')(conv3d_3)\n",
    "maxpool_3 = layers.MaxPool3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same', name='maxpool_3')(batch_3)\n",
    "\n",
    "# Flatten layer\n",
    "flatten = layers.Flatten()(maxpool_3)\n",
    "\n",
    "# Dense layers\n",
    "dense_1 = layers.Dense(units=16, activation='relu')(flatten)\n",
    "dense_2 = layers.Dense(units=16, activation='relu')(dense_1)\n",
    "\n",
    "# Output layer (change num_of_output as per your problem)\n",
    "output_layer = layers.Dense(num_of_output, activation=\"softmax\")(dense_2)\n",
    "\n",
    "# Model compilation\n",
    "eeg_model = models.Model(inputs=input_1, outputs=output_layer)\n",
    "\n",
    "# Compile the model with loss and optimizer\n",
    "eeg_model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])  # Change loss if binary classification\n",
    "\n",
    "# Model summary\n",
    "eeg_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping callback to stop training if validation loss does not improve\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Reduce learning rate when the validation loss plateaus\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train the model with training and validation data\n",
    "history = eeg_model.fit(\n",
    "    x=X_train, y=y_train,\n",
    "    batch_size=8,\n",
    "    epochs=20,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc)\n",
    "plt.title(\"Accuracy vs Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title(\"Loss vs Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = eeg_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred)) \n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"A\", \"B\"])\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_model.save(\"eeg_model_100_all.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
